{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate QA Agent - Demo Notebook\n",
    "\n",
    "This notebook demonstrates the Climate QA Agent system, which combines:\n",
    "- **Multi-turn conversation** with memory\n",
    "- **Retrieval-Augmented Generation (RAG)** for documentation queries\n",
    "- **Climate data analysis tools** for numerical computations\n",
    "- **Router-based agent** for automatic capability selection\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before running this notebook:\n",
    "1. Install dependencies: `pip install -r ../requirements.txt`\n",
    "2. Set up your `.env` file with `HF_TOKEN`\n",
    "3. Download data files (see `../data/README.md`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), '.env'))\n",
    "\n",
    "# Verify HF_TOKEN is set (don't print the actual token!)\n",
    "assert os.getenv('HF_TOKEN'), \"Please set HF_TOKEN in your .env file\"\n",
    "print(\"Environment loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Basic LLM Chat\n",
    "\n",
    "Load the model and demonstrate single-turn chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm import load_model, generate_response\n",
    "\n",
    "# Load model (this may take a moment)\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-turn chat example\n",
    "response = generate_response(\"Please introduce yourself in one sentence.\")\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Conversation Memory\n",
    "\n",
    "Demonstrate multi-turn conversation with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.memory import hf_chat, create_chat_session\n",
    "\n",
    "# Create a new chat session\n",
    "chat_history = create_chat_session()\n",
    "\n",
    "# First turn\n",
    "response1 = hf_chat(\"What is artificial intelligence?\", chat_history)\n",
    "print(\"User: What is artificial intelligence?\")\n",
    "print(\"Assistant:\", response1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up (uses conversation context)\n",
    "response2 = hf_chat(\"Summarize your previous answer in one sentence.\", chat_history)\n",
    "print(\"User: Summarize your previous answer in one sentence.\")\n",
    "print(\"Assistant:\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Demonstrate document retrieval and citation-based answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag import build_vectorstore, rag_answer\n",
    "\n",
    "# Build the vector store (loads corpus and creates embeddings)\n",
    "vectorstore = build_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG query example\n",
    "result = rag_answer(\"What does t2m mean and what unit is it in?\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another RAG query\n",
    "result = rag_answer(\"What is the spatial and temporal coverage of the dataset?\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Climate Data Analysis Tools\n",
    "\n",
    "Demonstrate the computational tools for NetCDF data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import load_datasets, inspect_dataset, compute_stat\n",
    "\n",
    "# Load datasets\n",
    "datasets = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a dataset\n",
    "info = inspect_dataset(\"t2m\")\n",
    "print(\"Dataset Info:\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "print(\"Mean temperature (Celsius):\")\n",
    "print(compute_stat(\"t2m\", metric=\"mean\", units=\"C\"))\n",
    "\n",
    "print(\"\\nTotal precipitation:\")\n",
    "print(compute_stat(\"tp\", metric=\"sum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Complete Agent System\n",
    "\n",
    "Demonstrate the full router-based agent that automatically selects capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent import run_agent\n",
    "import json\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What does t2m mean and what unit is it measured in?\",\n",
    "    \"What is the average temperature (t2m) in Celsius?\",\n",
    "    \"What does t2m mean and what is its average value in January?\",\n",
    "    \"What is the temporal coverage of the dataset and what is the total precipitation?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent on test questions\n",
    "for q in test_questions:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Question: {q}\")\n",
    "    \n",
    "    result = run_agent(q)\n",
    "    \n",
    "    print(\"\\nRouter Plan:\")\n",
    "    print(json.dumps(result[\"plan\"], indent=2))\n",
    "    \n",
    "    print(\"\\nFinal Answer:\")\n",
    "    print(result[\"final_answer\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Mode\n",
    "\n",
    "Try your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom question\n",
    "your_question = \"What is the maximum temperature in the dataset?\"\n",
    "\n",
    "result = run_agent(your_question)\n",
    "print(f\"Question: {your_question}\")\n",
    "print(f\"\\nPlan: {json.dumps(result['plan'], indent=2)}\")\n",
    "print(f\"\\nAnswer: {result['final_answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
